defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

prob:
  name: indepset
  prefix: ind
  n_objs: 3
  n_vars: 100
  attach: 4
  seed: 7

size:

# Distributed training
multi_gpu: false
env: local
dist_backend: nccl
init_method: tcp://127.0.0.1:3456
world_size: 1

# Training config
# scratch | last_checkpoint
training_from: scratch
logging: 2
# Epochs after which to validate the model
validate_every: 1
# Epochs after which to save the model
save_every: 1
# Set this to a positive number when using GPU
n_worker_dataloader: 1

# Dataset config
dataset:
  train:
    from_pid: 0
    to_pid: 10
    # Fraction of samples to use per training epoch in range (0, 1]
    per_epoch: 1
  val:
    from_pid: 1000
    to_pid: 1010

  # Save parents of nodes in the dataset
  with_parent: false
  # Number of negative samples per one positive sample
  neg_to_pos_ratio: 2
  layer_weight: exponential

# Optimization config
epochs: 3
batch_size: 100
opt:
  name: Adam
  lr: 1e-3
  wd: 1e-4
clip_grad: 1.0
norm_type: 2.0
agg: sum            # cat | sum


# Model config
# Graph_enc
model: transformer
d_emb: 64
top_k: 5
n_blocks: 2
n_heads: 8
dropout_token: 0
dropout: 0.2
bias_mha: false
bias_mlp: false
h2i_ratio: 2

# Problem
problem_type: 2
# Is problem is maximization form
maximization: true
# Graph type for Set Packing/Independent Set problem
graph_type: stidsen

hydra:
  output_subdir: null
  run:
    dir: .
