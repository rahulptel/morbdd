defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

seed: 1413
# Problem config
prob:
  name: indepset
  prefix: ind
  n_objs: 3
  n_vars: 100
  attach: 4
  seed: 7
# Size of the problem. To be set during execution
size:

# Raw data config used to generate the dataset
raw_data:
  # Save parents of nodes in the dataset
  with_parent: false

# Dataset config
dataset:
  version: 1
  train:
    from_pid: 0
    to_pid: 10
    # Fraction of samples to use per training epoch in range (0, 1]
    frac_per_epoch: 1
  val:
    from_pid: 1000
    to_pid: 1010
  test:
    from_pid: 1100
    to_pid: 1200
  # Number of negative samples per one positive sample
  neg_to_pos_ratio: 1

# Distributed training config
distributed: false
dist_backend: nccl
init_method: tcp://localhost:3456

# Training config
# scratch: Start afresh
# resume: Last checkpoint
init_from: scratch
# Validation performed per epoch | iter
train_mode: epoch
# Number of training epochs
epochs: 3
# Number of training iterations
max_iter: 10
# Epochs after which to validate the model if train_mode=epoch
validate_every_epoch: 1
# Iterations after which to validate the model if train_mode=iter
validate_every_iter: 10
# Perform validation on the master process
validate_on_master: true
# Epochs after which to save the model
save_every: 1
# Number of workers for dataloader. Set this to a positive number when using GPU
n_worker: 1
logging: 2

wandb:
  # Log to wandb
  log: false
  # wandb project name
  project: morbdd
  # Track the gradient of the model parameters
  track_grad: false
# Checkpoint directory with timestamp
with_timestamp: false

# Model config
# transformer | gat
encoder_type: transformer
n_node_feat: 2
n_edge_type: 2
d_emb: 64
top_k: 5
n_blocks: 2
n_heads: 8
dropout_token: 0
dropout: 0.2
bias_mha: false
bias_mlp: false
h2i_ratio: 2

# Optimization config
batch_size: 100
opt:
  name: Adam
  lr: 1e-3
  wd: 1e-4
clip_grad: 1.0
norm_type: 2.0
agg: sum            # cat | sum

# Loss function
# bce: binary cross entropy
# contrast: contrastive loss
loss: bce
# Weight nodes of the BDD based on the layer
layer_weight: exponential

eval:
  # learning | downstream
  task: learning
  # learning: train | val
  # downstream: train | val | test
  split: val
  # Select all nodes upto layer
  select_all: 1
  # Method used for stitching
  # parent: Select all nodes with a high-scoring parent
  # min_resistance: Minimize local resistance based on lookahead
  # mip: Minimize global resistance using MIP
  stitch: parent
  # To be used with min resistance
  lookahead: 1

# Problem
problem_type: 2
# Is problem is maximization form
maximization: true
# Graph type for Set Packing/Independent Set problem
graph_type: stidsen

hydra:
  output_subdir: null
  run:
    dir: .
